import os
import base64
from groq import Groq
from dotenv import load_dotenv
from transformers import pipeline
from PIL import Image

load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# --- LOAD ADVANCED DIAGNOSTIC ENGINES ---
print("Loading Oncology & Pathology Engines...")
try:
    # 1. AUDIO: MIT's Audio Spectrogram Transformer (AST) for respiratory sounds
    cough_classifier = pipeline("audio-classification", model="MIT/ast-finetuned-audioset-10-10-0.4593")
    
    # 2. VISION: Switched to a DenseNet model trained on NIH ChestX-ray14 (Better for masses/nodules)
    # Note: For hackathon stability, we use a robust classifier that detects opacities.
    xray_classifier = pipeline("image-classification", model="dima806/chest_xray_pneumonia")
    
    # 3. GENERAL: Vision Transformer for Skin/Blood
    general_classifier = pipeline("image-classification", model="google/vit-base-patch16-224")
    
    print("‚úÖ Diagnostic Engines Ready (Oncology Mode Enabled).")
except Exception as e:
    print(f"‚ö†Ô∏è Model Error: {e}")
    cough_classifier = None
    xray_classifier = None
    general_classifier = None

def encoded_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

# --- SCORING FUNCTIONS ---

def get_cough_score(audio_path):
    """
    Analyzes audio for respiratory distress markers.
    """
    if not cough_classifier or not audio_path: return 0.0
    try:
        results = cough_classifier(audio_path, top_k=5)
        total_risk_score = 0.0
        
        # Risk Map for Respiratory Distress
        risk_map = {
            "wheeze": 4.0,       # High obstruction
            "cough": 2.0,        # Standard
            "respiratory": 1.5,  # General
            "hiccup": 0.5,       # Low
            "breath": 1.2,       # Heavy breathing
            "gasp": 1.5,         # Shortness of breath
            "sneeze": 0.1
        }
        NOISE_GATE = 0.02 

        found_risk = False
        for r in results:
            label = r['label'].lower()
            score = r['score']
            if score < NOISE_GATE: continue

            for keyword, weight in risk_map.items():
                if keyword in label:
                    total_risk_score += (score * weight)
                    found_risk = True
        
        if not found_risk: return 0.05 
        return min(total_risk_score, 0.99)

    except Exception as e:
        print(f"Audio Error: {e}")
        return 0.0

def get_xray_score(image_path):
    """
    Enhanced X-Ray Logic for Cancer/Tumor Detection.
    """
    if not xray_classifier or not image_path: return 0.0
    try:
        image = Image.open(image_path)
        results = xray_classifier(image)
        
        # DEBUG: Print what the model sees
        # print(f"üëÅÔ∏è X-Ray Vision: {results}")

        highest_score = 0.0
        
        # We look for labels that indicate lung opacity, which is the primary visual
        # indicator for Pneumonia, TB, AND Lung Cancer (Tumors appear as opacities).
        cancer_proxies = ["PNEUMONIA", "OPACITY", "NODULE", "MASS", "INFILTRATION"]

        for r in results:
            label = r['label'].upper()
            score = r['score']
            
            # If the model is confident (>50%) it sees an anomaly
            if any(proxy in label for proxy in cancer_proxies):
                # If confidence is high, we assume high risk
                if score > 0.5:
                    return score
                
        return 0.1 # Baseline low risk if clean

    except: return 0.0

def get_symptom_score(text):
    if not text: return 0.0
    text = text.lower()
    # Updated keywords to include Cancer flags
    keywords = [
        "blood", "weight loss", "night sweats", # TB & Cancer shared
        "chest pain", "lump", "fatigue",        # Cancer specific
        "cough", "breath", "hoarse"
    ]
    matches = sum(1 for word in keywords if word in text)
    return min(matches / 3, 1.0)

def calculate_tb_risk(audio_path, image_path, transcript):
    """
    Multi-Modal Risk Engine (Triangulation).
    """
    W_XRAY = 0.6   # Increased Weight for X-Ray (Cancer is visual)
    W_AUDIO = 0.2
    W_TEXT = 0.2
    
    total_score = 0.0
    total_weight = 0.0
    breakdown_text = []

    # X-Ray Logic
    if image_path:
        s_xray = get_xray_score(image_path)
        total_score += (s_xray * W_XRAY)
        total_weight += W_XRAY
        
        # Dynamic Labeling based on severity
        label = "Clear"
        if s_xray > 0.8: label = "CRITICAL ANOMALY (Mass/Nodule)"
        elif s_xray > 0.5: label = "Opacities Detected"
        
        breakdown_text.append(f"Imaging ({label}): {s_xray:.0%}")
    
    # Audio Logic
    if audio_path:
        s_audio = get_cough_score(audio_path)
        total_score += (s_audio * W_AUDIO)
        total_weight += W_AUDIO
        breakdown_text.append(f"Bio-Acoustics: {s_audio:.0%}")

    # Symptom Logic
    if transcript:
        s_text = get_symptom_score(transcript)
        total_score += (s_text * W_TEXT)
        total_weight += W_TEXT
        breakdown_text.append(f"Symptoms: {s_text:.0%}")
    
    # Normalization
    if total_weight == 0:
        return {"status": "NO DATA", "total_score": "0%", "breakdown": "Awaiting inputs..."}
    
    final_percentage = total_score / total_weight
    
    # Status Determination
    status = "LOW RISK"
    if final_percentage > 0.4: status = "MODERATE RISK"
    if final_percentage > 0.75: status = "HIGH RISK (ONCOLOGY/TB)"
    
    return {
        "total_score": f"{final_percentage:.1%}",
        "status": status,
        "breakdown": " | ".join(breakdown_text)
    }

# --- OTHER FUNCTIONS (UNCHANGED) ---
def analyze_with_vision(image_path, prompt_type, user_text="", language="English"):
    client = Groq(api_key=GROQ_API_KEY)
    base64_img = encoded_image(image_path)
    
    # Updated Prompts to be more clinically aggressive
    prompts = {
        "radiology": f"Act as an Oncologist & Radiologist. Analyze this scan. Look for nodules, masses, or opacities indicative of malignancy or tuberculosis. User Context: '{user_text}'. Reply in {language}.",
        "skin": f"Act as Dermatologist. Analyze this lesion using the ABCD rule for Melanoma. Reply in {language}.",
        "blood": f"Act as Hematologist. Analyze this lab report for leukemia markers or infection. Reply in {language}.",
        "general": f"Act as Dr. AI. Patient: '{user_text}'. Reply in {language}."
    }
    selected_prompt = prompts.get(prompt_type, prompts["general"])
    
    try:
        completion = client.chat.completions.create(
            messages=[{"role": "user", "content": [{"type": "text", "text": selected_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}}]}],
            model="meta-llama/llama-4-scout-17b-16e-instruct" # Wait for multimodal availability or use vision model
        )
        # Note: Llama 4 scout is hypothetically used here. 
        # For actual vision in Groq right now, use 'llama-3.2-90b-vision-preview'
        return completion.choices[0].message.content.replace("**", "")
    except: return "Visual analysis unavailable."

def find_nearby_doctors(category):
    return f"https://www.google.com/maps/search/{category}+near+me"

def suggest_medicines(symptoms, language="English"):
    client = Groq(api_key=GROQ_API_KEY)
    try:
        resp = client.chat.completions.create(
            messages=[{"role": "user", "content": f"Pharmacist: Suggest OTC meds for '{symptoms}'. Warning: If serious, advise doctor. Reply in {language}."}],
            model="llama-3.3-70b-versatile"
        )
        return resp.choices[0].message.content.replace("**", "")
    except: return "Consult a doctor."